{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR BETWEEN PREDICTED AND ACTUAL VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is usually for the scraping part. I will be scraping according to the dates. This is because I want to deal with less data in the beginning and will try to implement the code on a larger scale.\n",
    "# The date I have chosen is 30th April 2019.\n",
    "import pandas as pd\n",
    "import ftputil\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "data = { 'Selection criteria': ['time range'], 'index_history.txt': [' ✔'], 'index_monthly.txt': [' ✔'],'index_latest.txt': [' ✔']}\n",
    "pd.DataFrame(data=data)\n",
    "output_directory = os.getcwd()\n",
    "user = 'anathsharma'\n",
    "password = '@n&1!MZx'\n",
    "product_name = 'INSITU_BAL_NRT_OBSERVATIONS_013_032'  \n",
    "host = 'nrt.cmems-du.eu' \n",
    "index_file = 'index_latest.txt'\n",
    "date_format = \"%Y-%m-%dT%H:%M:%SZ\" \n",
    "ini = datetime.datetime.strptime('2019-04-30T00:00:00Z', date_format)\n",
    "end = datetime.datetime.strptime('2019-04-30T23:59:59Z', date_format)\n",
    "#We can scrape the data for as long as it is required and simultaneous data can be inserted in the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\awshesh nath sharma\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `use_list_a_option` will default to `False` in ftputil 4.x.x\n",
      "  \n",
      "c:\\users\\awshesh nath sharma\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:8: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# This part is for developing the intermediate between my laptop and the host server from where the data is scraped.\n",
    "#I used the help of Marine \n",
    "#connect to CMEMS FTP\n",
    "with ftputil.FTPHost(host, user, password) as ftp_host: \n",
    "    \n",
    "    #open the index file to read\n",
    "    with ftp_host.open(\"Core\"+'/'+product_name+'/'+index_file, \"r\") as indexfile:\n",
    "        \n",
    "        #read the index file as a comma-separate-value file\n",
    "        index = np.genfromtxt(indexfile, skip_header=6, unpack=False, delimiter=',', dtype=None, names=['catalog_id', 'file_name','geospatial_lat_min', 'geospatial_lat_max', 'geospatial_lon_min','geospatial_lon_max','time_coverage_start', 'time_coverage_end', 'provider', 'date_update', 'data_mode', 'parameters'])\n",
    "               \n",
    "        #loop over the lines/netCDFs and download the most sutable ones for you\n",
    "        for netCDF in index:\n",
    "            \n",
    "            #getting ftplink, filepath and filename\n",
    "            ftplink = netCDF['file_name'].decode('utf-8')\n",
    "            filepath = '/'.join(ftplink.split('/')[3:len(ftplink.split('/'))])\n",
    "            ncdf_file_name = ftplink[ftplink.rfind('/')+1:]\n",
    "            \n",
    "            #download netCDF if meeting selection criteria\n",
    "            time_start = datetime.datetime.strptime(netCDF['time_coverage_start'].decode('utf-8'), date_format)\n",
    "            time_end = datetime.datetime.strptime(netCDF['time_coverage_start'].decode('utf-8'), date_format)\n",
    "            if time_start > ini  and time_end < end: \n",
    "                if ftp_host.path.isfile(filepath):\n",
    "                    cwd = os.getcwd()\n",
    "                    os.chdir(output_directory)\n",
    "                    ftp_host.download(filepath, ncdf_file_name)  # remote, local\n",
    "                    os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading into csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reading the actual insitu observations which is scraped from the site.\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "file = Dataset(\"BO_LATEST_TS_FB_FinnMaid_20190430.nc\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF3_CLASSIC data model, file format NETCDF3):\n",
      "    naming_authority: Copernicus\n",
      "    area: Baltic Sea\n",
      "    format_version: 1.2\n",
      "    contact: cmems-service@smhi.se\n",
      "    author: cmems-service\n",
      "    cdm_data_type: Station\n",
      "    summary: Oceanographic data from the Baltic Sea. Measured properties: the hydrographic conditions as currents, temperature and salinity.\n",
      "    references: http://www.oceansites.org http://www.myocean.org\n",
      "    data_assembly_center: Baltic INS TAC DU\n",
      "    update_interval: daily\n",
      "    citation: These data were collected and made freely available by the Copernicus project and the programs that contribute to it\n",
      "    Conventions: CF-1.6 OceanSITES-Manual-1.2 Copernicus-InSituTAC-SRD-1.3 Copernicus-InSituTAC-ParametersList-3.1.0\n",
      "    qc_manual: OceanSITES User's Manual v1.1\n",
      "    distribution_statement: These data follow Copernicus standards; they are public and free of charge. User assumes all risk for use of data. User must display citation in any publication or product using data. User must contact PI prior to any commercial use of data.\n",
      "    quality_index: A\n",
      "    comment: None\n",
      "    netcdf_version: 3.5\n",
      "    source: vessel of opportunity on fixed route\n",
      "    geospatial_lon_min: 10.9012\n",
      "    quality_control_indicator: 1\n",
      "    geospatial_lon_max: 14.3849\n",
      "    data_type: OceanSITES time-series data\n",
      "    platform_code: FinnMaid\n",
      "    geospatial_vertical_min: 5.0000\n",
      "    time_coverage_end: 2019-04-30T09:00:20Z\n",
      "    id: BO_LATEST_TS_FB_FinnMaid_20190430\n",
      "    last_date_observation: 2019-04-30T09:00:20Z\n",
      "    time_coverage_start: 2019-04-30T02:12:13Z\n",
      "    history: 2019-04-30T05:41:12Z File created by SMHI Baltic Sea PU.\n",
      "2019-04-30T13:01:17Z Data converted by SMHI Baltic Sea PU.\n",
      "    geospatial_vertical_max: 5.0000\n",
      "    institution_references: seppo.kaitala@ymparisto.fi\n",
      "    last_longitude_observation: 14.3849\n",
      "    pi_name: SYKE\n",
      "    geospatial_lat_min: 53.9691\n",
      "    wmo_platform_code: OJMI\n",
      "    geospatial_lat_max: 55.1961\n",
      "    institution_edmo_code: 1104\n",
      "    data_mode: R\n",
      "    site_code: FinnMaid\n",
      "    title: Baltic - NRT in situ Observations\n",
      "    date_update: 2019-04-30T13:01:17Z\n",
      "    last_latitude_observation: 55.1961\n",
      "    institution: SYKE\n",
      "    dimensions(sizes): TIME(1144), LATITUDE(1144), LONGITUDE(1144), POSITION(1144), DEPTH(1)\n",
      "    variables(dimensions): float64 \u001b[4mTIME\u001b[0m(TIME), int8 \u001b[4mTIME_QC\u001b[0m(TIME), float32 \u001b[4mLATITUDE\u001b[0m(LATITUDE), float32 \u001b[4mLONGITUDE\u001b[0m(LONGITUDE), int8 \u001b[4mPOSITION_QC\u001b[0m(POSITION), float32 \u001b[4mPSAL\u001b[0m(TIME,DEPTH), int8 \u001b[4mPSAL_QC\u001b[0m(TIME,DEPTH), float32 \u001b[4mTEMP\u001b[0m(TIME,DEPTH), int8 \u001b[4mTEMP_QC\u001b[0m(TIME,DEPTH), float32 \u001b[4mDEPH\u001b[0m(TIME,DEPTH), int8 \u001b[4mDEPH_QC\u001b[0m(TIME,DEPTH), float32 \u001b[4mFLU2\u001b[0m(TIME,DEPTH), int8 \u001b[4mFLU2_QC\u001b[0m(TIME,DEPTH)\n",
      "    groups: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finding the features of the file \n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Features of the File \n",
    "d=file.variables[\"LATITUDE\"][:]\n",
    "e=file.variables[\"LONGITUDE\"][:]\n",
    "#f=file.variables[\"POSITION_QC\"][:]\n",
    "g=file.variables[\"TEMP\"][:]\n",
    "h=file.variables[\"DEPH\"][:]\n",
    "po=file.variables[\"TIME\"][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25321.09181713 25321.09206019 25321.09230324 ... 25321.37474537\n",
      " 25321.37498843 25321.37523148]\n"
     ]
    }
   ],
   "source": [
    "print(po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('depth', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 depth(depth)\n",
      "    standard_name: depth\n",
      "    long_name: depth\n",
      "    units: m\n",
      "    positive: down\n",
      "    axis: Z\n",
      "    unit_long: meters\n",
      "    _CoordinateAxisType: Height\n",
      "    _CoordinateZisPositive: down\n",
      "    valid_min: 0.0\n",
      "    valid_max: 5.0\n",
      "unlimited dimensions: \n",
      "current shape = (2,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "), ('sob', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 sob(time, lat, lon)\n",
      "    standard_name: sea_water_salinity\n",
      "    long_name: Sea water salinity at sea floor\n",
      "    units: 0.001\n",
      "    _FillValue: -999.0\n",
      "    missing_value: -999.0\n",
      "    unit_long: practical salinity unit\n",
      "    _ChunkSizes: [  1 773 767]\n",
      "unlimited dimensions: \n",
      "current shape = (2, 38, 3)\n",
      "filling on), ('vo', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 vo(time, depth, lat, lon)\n",
      "    long_name: Northward current\n",
      "    units: m s-1\n",
      "    _FillValue: -999.0\n",
      "    missing_value: -999.0\n",
      "    unit_long: meters per second\n",
      "    standard_name: northward_sea_water_velocity\n",
      "    _ChunkSizes: [  1   1 773 767]\n",
      "unlimited dimensions: \n",
      "current shape = (2, 2, 38, 3)\n",
      "filling on), ('lon', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 lon(lon)\n",
      "    standard_name: longitude\n",
      "    long_name: longitude\n",
      "    units: degrees_east\n",
      "    axis: X\n",
      "    _CoordinateAxisType: Lon\n",
      "    valid_min: 13.875\n",
      "    valid_max: 13.930555\n",
      "unlimited dimensions: \n",
      "current shape = (3,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "), ('thetao', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 thetao(time, depth, lat, lon)\n",
      "    standard_name: sea_water_potential_temperature\n",
      "    long_name: Potential temperature\n",
      "    units: degrees_C\n",
      "    _FillValue: -999.0\n",
      "    missing_value: -999.0\n",
      "    unit_long: degree Celsius\n",
      "    _ChunkSizes: [  1   1 773 767]\n",
      "unlimited dimensions: \n",
      "current shape = (2, 2, 38, 3)\n",
      "filling on), ('uo', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 uo(time, depth, lat, lon)\n",
      "    standard_name: eastward_sea_water_velocity\n",
      "    long_name: Eastward current\n",
      "    units: m s-1\n",
      "    _FillValue: -999.0\n",
      "    missing_value: -999.0\n",
      "    unit_long: meters per second\n",
      "    _ChunkSizes: [  1   1 773 767]\n",
      "unlimited dimensions: \n",
      "current shape = (2, 2, 38, 3)\n",
      "filling on), ('time', <class 'netCDF4._netCDF4.Variable'>\n",
      "float64 time(time)\n",
      "    standard_name: time\n",
      "    long_name: time\n",
      "    bounds: time_bnds\n",
      "    units: days since 1900-01-01 00:00:00\n",
      "    calendar: standard\n",
      "    axis: T\n",
      "    _ChunkSizes: 512\n",
      "    _CoordinateAxisType: Time\n",
      "    valid_min: 43582.520833333336\n",
      "    valid_max: 43583.520833333336\n",
      "unlimited dimensions: \n",
      "current shape = (2,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "), ('bottomT', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 bottomT(time, lat, lon)\n",
      "    standard_name: sea_water_potential_temperature_at_sea_floor\n",
      "    long_name: Sea water potential temperature at sea floor (given for depth comprise between 0 and 500m)\n",
      "    units: degrees_C\n",
      "    _FillValue: -999.0\n",
      "    missing_value: -999.0\n",
      "    unit_long: degree Celsius\n",
      "    _ChunkSizes: [  1 773 767]\n",
      "unlimited dimensions: \n",
      "current shape = (2, 38, 3)\n",
      "filling on), ('so', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 so(time, depth, lat, lon)\n",
      "    standard_name: sea_water_salinity\n",
      "    long_name: Salinity\n",
      "    units: 0.001\n",
      "    _FillValue: -999.0\n",
      "    missing_value: -999.0\n",
      "    unit_long: practical salinity unit\n",
      "    _ChunkSizes: [  1   1 773 767]\n",
      "unlimited dimensions: \n",
      "current shape = (2, 2, 38, 3)\n",
      "filling on), ('lat', <class 'netCDF4._netCDF4.Variable'>\n",
      "float32 lat(lat)\n",
      "    standard_name: latitude\n",
      "    long_name: latitude\n",
      "    units: degrees_north\n",
      "    axis: Y\n",
      "    _CoordinateAxisType: Lat\n",
      "    valid_min: 54.258327\n",
      "    valid_max: 54.874996\n",
      "unlimited dimensions: \n",
      "current shape = (38,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      ")])\n",
      "2\n",
      "[43582.52083333 43583.52083333]\n"
     ]
    }
   ],
   "source": [
    "# Reading the Forecasted the Dataset\n",
    "# The forecasted dataset can be downloaded by applying the filters on the website.\n",
    "#The feature used for comparison of the \n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "file1 = Dataset(\"dataset-bal-analysis-forecast-phy-dailymeans_1556710689517.nc\",'r')\n",
    "print(file1.variables)\n",
    "ba=file1.variables[\"depth\"][:]\n",
    "da=file1.variables[\"lat\"][:]\n",
    "ea=file1.variables[\"thetao\"][:]\n",
    "fa=file1.variables[\"lon\"][:]\n",
    "i=0\n",
    "j=0\n",
    "k=0\n",
    "aw=[]\n",
    "bc=[]\n",
    "cd=[]\n",
    "while(i<len(ea)):\n",
    "    b=np.ma.mean(ea[i])\n",
    "    if (b==\"nan\"):\n",
    "        aw.append(\"nan\")\n",
    "    else:    \n",
    "        aw.append(b)\n",
    "    i+=1\n",
    "print((len(aw))) \n",
    "po=file1.variables[\"time\"][:]\n",
    "print(po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for the conversion to the list format for better interpretation.  \n",
    "aw=[]\n",
    "aa=[]\n",
    "ab=[]\n",
    "ac=[]\n",
    "ad=[]\n",
    "m=0\n",
    "while(m<len(po)):\n",
    "    k=0\n",
    "    while(k<len(ba)):\n",
    "        j=0\n",
    "        while(j<len(da)):\n",
    "            i=0\n",
    "            while(i<len(fa)):\n",
    "                aw.append(ea[m,k,j,i])\n",
    "                aa.append(fa[i])\n",
    "                ab.append(da[j])\n",
    "                ac.append(ba[k])\n",
    "                ad.append(po[m])\n",
    "                i+=1\n",
    "            j+=1 \n",
    "        k+=1 \n",
    "    m+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For writing into csv format.\n",
    "import pandas as pd\n",
    "df_result=pd.DataFrame(list(zip(ad,ac,ab,aa,aw)),columns=['Time','Depth',\"Latitude\",\"Longitude\",\"Temperature\"])\n",
    "df_result.to_csv('Testtemp'+('.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[11.4]', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2', '7.2']\n",
      "[54.25, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078, 54.88330078]\n",
      "[13.92000008, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017, 13.86670017]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "fil=pd.read_csv(\"TempGreifswalderoi.csv\")\n",
    "Temp=fil['Temperature']\n",
    "lati=fil['Latitude']\n",
    "long=fil['Longitude']\n",
    "Date=fil['Time']\n",
    "aw=[]\n",
    "ty=set(lati)\n",
    "tu=set(long)\n",
    "i=0\n",
    "uy=[]\n",
    "aw=[]\n",
    "am=[]\n",
    "at=[]\n",
    "at.append\n",
    "while(i<(len(lati)-1)):\n",
    "    if lati[i]==lati[i+1]:\n",
    "        j=0\n",
    "        while(j<(len(long)-1)):\n",
    "            if long[j]==long[j+1]:\n",
    "                at.append(Temp[j])\n",
    "                aw.append(long[j])\n",
    "                am.append(lati[j])\n",
    "            else:    \n",
    "                at.append('nan')\n",
    "                aw.append('nan')\n",
    "                am.append('nan')\n",
    "            j+=1\n",
    "    i+=1    \n",
    "t=0\n",
    "er=[0]\n",
    "while(t<(len(at))):\n",
    "    if aw[t]=='nan':\n",
    "        er.append(t)\n",
    "    t+=1\n",
    "r=0\n",
    "pl=[]\n",
    "oi=[]\n",
    "po=[]\n",
    "t=0\n",
    "while(t<(len(er)-1)):\n",
    "    qw=at[(er[t]):(er[t+1])]\n",
    "    ty=am[(er[t]):(er[t+1])]\n",
    "    ui=aw[(er[t]):(er[t+1])]\n",
    "    rt=0\n",
    "    il=[]\n",
    "    y=0\n",
    "    while(rt<len(qw)):\n",
    "        if qw[rt]=='nan':\n",
    "            il.append(rt)\n",
    "        rt+=1\n",
    "    \n",
    "    while(y<len(il)):\n",
    "        qw.pop(il[y]) \n",
    "        y+=1\n",
    "    pl.append(qw[0])\n",
    "    y=0\n",
    "    while(rt<len(ty)):\n",
    "        if ty[rt]=='nan':\n",
    "            il.append(rt)\n",
    "        rt+=1\n",
    "    \n",
    "    while(y<len(il)):\n",
    "        ty.pop(il[y]) \n",
    "        y+=1\n",
    "    po.append(ty[0])\n",
    "    y=0\n",
    "    while(rt<len(ui)):\n",
    "        if ui[rt]=='nan':\n",
    "            il.append(rt)\n",
    "        rt+=1\n",
    "    \n",
    "    while(y<len(il)):\n",
    "        ui.pop(il[y]) \n",
    "        y+=1\n",
    "    oi.append(ui[0]) \n",
    "    t+=1\n",
    "print(pl)\n",
    "print(po)\n",
    "print(oi)\n",
    "#We need to convert our oi to list format.\n",
    "#We load different dates datasets and find out the number of different values.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2\n"
     ]
    }
   ],
   "source": [
    "test_list = pl\n",
    "res = max(set(test_list), key = test_list.count)\n",
    "print(res)\n",
    "#We can find out the value of temperature of maximum frequency of a particular day.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to SQL Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This can be used for the conversion of csv files to sqlite Database\n",
    "from __future__ import print_function\n",
    "import sqlite3\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    " \n",
    "db = sys.argv[1]\n",
    " \n",
    "conn = sqlite3.connect(db)\n",
    "conn.text_factory = str  # allows utf-8 data to be stored\n",
    " \n",
    "c = conn.cursor()\n",
    " \n",
    "# traverse the directory and process each .csv file\n",
    "for csvfile in glob.glob(os.path.join(sys.argv[2], \"*.csv\")):\n",
    "    # remove the path and extension and use what's left as a table name\n",
    "    tablename = os.path.splitext(os.path.basename(csvfile))[0]\n",
    " \n",
    "    with open(csvfile, \"rb\") as f:\n",
    "        reader = csv.reader(f)\n",
    " \n",
    "        header = True\n",
    "        for row in reader:\n",
    "            if header:\n",
    "                # gather column names from the first row of the csv\n",
    "                header = False\n",
    " \n",
    "                sql = \"DROP TABLE IF EXISTS %s\" % tablename\n",
    "                c.execute(sql)\n",
    "                sql = \"CREATE TABLE %s (%s)\" % (tablename,\n",
    "                          \", \".join([ \"%s text\" % column for column in row ]))\n",
    "                c.execute(sql)\n",
    " \n",
    "                for column in row:\n",
    "                    if column.lower().endswith(\"_id\"):\n",
    "                        index = \"%s__%s\" % ( tablename, column )\n",
    "                        sql = \"CREATE INDEX %s on %s (%s)\" % ( index, tablename, column )\n",
    "                        c.execute(sql)\n",
    " \n",
    "                insertsql = \"INSERT INTO %s VALUES (%s)\" % (tablename,\n",
    "                            \", \".join([ \"?\" for column in row ]))\n",
    " \n",
    "                rowlen = len(row)\n",
    "            else:\n",
    "                # skip lines that don't have the right number of columns\n",
    "                if len(row) == rowlen:\n",
    "                    c.execute(insertsql, row)\n",
    " \n",
    "        conn.commit()\n",
    " \n",
    "c.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8519784647644993\n"
     ]
    }
   ],
   "source": [
    "#Computation for the mean_squared_errors between forecasted dataset and the actual insitu observations.\n",
    "#Computation of the lists can be found out for different days as described in the above procedure.\n",
    "#l1=list for forecasted weather data\n",
    "#l2=list for actual insitu observations\n",
    "#The data is for two days.\n",
    "l1=[7.2,11.15]\n",
    "l2=[7.736523,9.96]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "aw=mean_squared_error(l1,l2)\n",
    "print(aw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, we can do the analysis for different parameters and at different depths."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
